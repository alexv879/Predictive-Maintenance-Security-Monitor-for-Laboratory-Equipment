{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Premonitor: train_models.py\n",
        "# This script is responsible for training the core AI models for the Premonitor project.\n",
        "# It should be run on a powerful development PC with a GPU.\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers, losses, models, layers, callbacks\n",
        "\n",
        "# Import our custom project files\n",
        "import config\n",
        "import model_blueprints\n",
        "import utils # This will be created next\n",
        "\n",
        "# --- Custom Loss Function for SimSiam Pre-training ---\n",
        "def sim_siam_loss(p, z):\n",
        "    \"\"\"Calculates the negative cosine similarity loss for the SimSiam model.\"\"\"\n",
        "    z = tf.stop_gradient(z) # Crucial step to prevent model collapse\n",
        "    p = tf.math.l2_normalize(p, axis=1)\n",
        "    z = tf.math.l2_normalize(z, axis=1)\n",
        "    return -tf.reduce_mean(tf.reduce_sum((p * z), axis=1))\n",
        "\n",
        "# --- Main Training Functions ---\n",
        "\n",
        "def train_thermal_model(epochs=50, batch_size=32):\n",
        "    \"\"\"\n",
        "    Orchestrates the full, two-stage training process for the thermal model.\n",
        "    Stage 1: Self-supervised pre-training on unlabeled data.\n",
        "    Stage 2: Supervised fine-tuning on labeled data.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Full Thermal Model Training Pipeline ---\")\n",
        "\n",
        "    # --- STAGE 1: SELF-SUPERVISED PRE-TRAINING ---\n",
        "    print(\"\\n--- STAGE 1: Self-Supervised Pre-training ---\")\n",
        "\n",
        "    # 1. Load Data for Pre-training (unlabeled images)\n",
        "    # This function in utils.py should load paths from AAU VAP and FLIR datasets.\n",
        "    unlabeled_image_paths = utils.load_all_thermal_image_paths()\n",
        "    train_dataset = utils.create_thermal_dataset_generator(unlabeled_image_paths, batch_size)\n",
        "\n",
        "    # 2. Get Model Blueprints\n",
        "    siamese_model, encoder, _ = model_blueprints.get_thermal_anomaly_model()\n",
        "    siamese_model.compile(optimizer=optimizers.Adam(0.001))\n",
        "\n",
        "    # 3. Custom Training Loop for SimSiam\n",
        "    print(f\"Starting SimSiam pre-training for {epochs} epochs...\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for step, (view1, view2) in enumerate(train_dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                p_a, z_a, p_b, z_b = siamese_model([view1, view2], training=True)\n",
        "                loss = (sim_siam_loss(p_a, z_b) + sim_siam_loss(p_b, z_a)) / 2\n",
        "\n",
        "            gradients = tape.gradient(loss, siamese_model.trainable_variables)\n",
        "            siamese_model.optimizer.apply_gradients(zip(gradients, siamese_model.trainable_variables))\n",
        "            total_loss += loss\n",
        "\n",
        "        avg_loss = total_loss / (step + 1)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Pre-training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 4. Save the Pre-trained Encoder\n",
        "    # This encoder now has a powerful understanding of thermal images.\n",
        "    encoder_save_path = os.path.join(config.MODEL_DIR, \"thermal_encoder_pretrained.h5\")\n",
        "    if not os.path.exists(config.MODEL_DIR): os.makedirs(config.MODEL_DIR)\n",
        "    encoder.save(encoder_save_path)\n",
        "    print(f\"Pre-trained encoder saved to {encoder_save_path}\")\n",
        "\n",
        "    # --- STAGE 2: SUPERVISED FINE-TUNING ---\n",
        "    print(\"\\n--- STAGE 2: Supervised Fine-tuning ---\")\n",
        "\n",
        "    # 1. Load Labeled Data for Fine-tuning\n",
        "    # This function in utils.py should load your small, custom-labeled dataset.\n",
        "    # It should return a dataset of (image, label) pairs.\n",
        "    labeled_train_ds, labeled_val_ds = utils.load_labeled_thermal_data(batch_size)\n",
        "\n",
        "    # 2. Build the Classifier Model\n",
        "    # Load the pre-trained encoder and freeze its backbone layers.\n",
        "    pretrained_encoder = models.load_model(encoder_save_path)\n",
        "    pretrained_encoder.trainable = False # Start with the backbone frozen\n",
        "\n",
        "    # Add a new classification head\n",
        "    classifier_input = layers.Input(shape=config.THERMAL_MODEL_INPUT_SHAPE)\n",
        "    x = pretrained_encoder(classifier_input, training=False)\n",
        "    # Add a dropout layer for regularization to prevent overfitting\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    # The final output layer for binary classification (normal vs anomaly)\n",
        "    classifier_output = layers.Dense(1, activation='sigmoid')(x)\n",
        "    classifier_model = models.Model(classifier_input, classifier_output, name=\"thermal_classifier\")\n",
        "\n",
        "    # 3. Compile and Train the Classifier\n",
        "    classifier_model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.0001), # Use a smaller learning rate for fine-tuning\n",
        "        loss=losses.BinaryCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 4. Add Checkpointing to Save the Best Model\n",
        "    # This saves the model only when validation accuracy improves.\n",
        "    checkpoint_path = os.path.join(config.MODEL_DIR, \"thermal_classifier_best.h5\")\n",
        "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=False,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "\n",
        "    print(f\"Starting classifier fine-tuning for {epochs // 2} epochs...\")\n",
        "    classifier_model.fit(\n",
        "        labeled_train_ds,\n",
        "        epochs=epochs // 2, # Fine-tuning usually requires fewer epochs\n",
        "        validation_data=labeled_val_ds,\n",
        "        callbacks=[model_checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    print(f\"Fine-tuning complete. Best model saved to {checkpoint_path}\")\n",
        "    # This 'thermal_classifier_best.h5' is the final model you will convert to .tflite\n",
        "\n",
        "def train_acoustic_model(epochs=30, batch_size=64):\n",
        "    \"\"\"Orchestrates the training for the acoustic anomaly model.\"\"\"\n",
        "    print(\"--- Starting Acoustic Model Training ---\")\n",
        "\n",
        "    # 1. Load Data (from MIMII, Urbansound8K, etc.)\n",
        "    # This function in utils.py will handle loading audio, creating spectrograms,\n",
        "    # and making the pseudo-anomaly pairs.\n",
        "    spectrograms, labels = utils.create_acoustic_dataset(data_dir='path/to/your/mimii/0_dB_fan/')\n",
        "\n",
        "    # 2. Get Model Blueprint\n",
        "    acoustic_model = model_blueprints.get_acoustic_anomaly_model()\n",
        "\n",
        "    # 3. Compile the Model\n",
        "    acoustic_model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss=losses.BinaryCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 4. Add Checkpointing\n",
        "    checkpoint_path = os.path.join(config.MODEL_DIR, \"acoustic_anomaly_model_best.h5\")\n",
        "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=False,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "\n",
        "    # 5. Training\n",
        "    print(f\"Starting acoustic model training for {epochs} epochs...\")\n",
        "    acoustic_model.fit(\n",
        "        spectrograms,\n",
        "        labels,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[model_checkpoint_callback]\n",
        "    )\n",
        "    print(f\"Training complete. Best model saved to {checkpoint_path}\")\n",
        "\n",
        "# --- Command-Line Interface ---\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Premonitor AI Model Training Script\")\n",
        "    parser.add_argument(\n",
        "        \"--model\", type=str, required=True, choices=[\"thermal\", \"acoustic\"],\n",
        "        help=\"The type of model to train ('thermal' or 'acoustic').\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create model directory if it doesn't exist\n",
        "    if not os.path.exists(config.MODEL_DIR):\n",
        "        os.makedirs(config.MODEL_DIR)\n",
        "\n",
        "    if args.model == \"thermal\":\n",
        "        # NOTE: For this to run, you must implement the data loading functions\n",
        "        # in utils.py: load_all_thermal_image_paths() and load_labeled_thermal_data()\n",
        "        print(\"Running in placeholder mode for thermal training.\")\n",
        "        # train_thermal_model()\n",
        "    elif args.model == \"acoustic\":\n",
        "        # NOTE: For this to run, you must implement create_acoustic_dataset() in utils.py\n",
        "        print(\"Running in placeholder mode for acoustic training.\")\n",
        "        # train_acoustic_model()\n",
        "    else:\n",
        "        print(\"Invalid model type specified.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "m0GOp5g6V8Be"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}