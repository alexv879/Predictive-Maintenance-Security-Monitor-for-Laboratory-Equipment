# -*- coding: utf-8 -*-
"""Premonitor_main_py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_kJeCXwqnqBhbRY6gpMoBr6u2Daqzkkn
"""

# Premonitor: main.py
# This is the main entry point for the Premonitor application.
# It runs a continuous loop to monitor the environment, processes data with
# AI models, applies fusion logic, and triggers non-blocking alerts.
# This script is intended to be run on the Raspberry Pi.

import time
import numpy as np
from datetime import datetime
import os
import json
import sys
import logging
from pathlib import Path

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('premonitor')

# Try importing tflite_runtime first (preferred on Pi), fallback to tensorflow
try:
    import tflite_runtime.interpreter as tflite
    logger.info("Using tflite_runtime for inference")
    USING_TFLITE_RUNTIME = True
except ImportError:
    try:
        import tensorflow as tf
        tflite = tf.lite
        logger.info("Using tensorflow.lite for inference")
        USING_TFLITE_RUNTIME = False
    except ImportError:
        logger.critical("Neither tflite_runtime nor tensorflow.lite could be imported")
        sys.exit(1)

# Import our custom project files
try:
    import config
    import alert_manager
    # For the MVP, we use the mock hardware. To switch to real hardware,
    # you would change this line to: import hardware_drivers as hardware
    import mock_hardware as hardware
    logger.info("Successfully imported project modules")
except ImportError as e:
    logger.critical(f"Failed to import project modules: {e}")
    sys.exit(1)

# --- Global variables for loaded AI models ---
thermal_interpreter = None
acoustic_interpreter = None
# Add other models like LSTM AE here in the future.

def startup_check():
    """
    Verify all required model paths, config entries, and dependencies before starting.
    Returns True if all checks pass, False otherwise.
    """
    logger.info("Running startup checks...")

    all_checks_passed = True

    # Check config module attributes
    required_config_attrs = [
        'THERMAL_MODEL_PATH',
        'ACOUSTIC_MODEL_PATH',
        'THERMAL_ANOMALY_CONFIDENCE',
        'ACOUSTIC_ANOMALY_CONFIDENCE',
        'SENSOR_READ_INTERVAL'
    ]

    for attr in required_config_attrs:
        if not hasattr(config, attr):
            logger.error(f"Missing required config attribute: {attr}")
            all_checks_passed = False
        else:
            logger.debug(f"Config check passed: {attr} = {getattr(config, attr)}")

    # Check model file paths exist
    model_paths = {
        'thermal': getattr(config, 'THERMAL_MODEL_PATH', ''),
        'acoustic': getattr(config, 'ACOUSTIC_MODEL_PATH', '')
    }

    for model_name, model_path in model_paths.items():
        if not model_path:
            logger.error(f"{model_name.upper()}_MODEL_PATH is not configured")
            all_checks_passed = False
        elif not os.path.exists(model_path):
            logger.error(f"{model_name.capitalize()} model not found: {model_path}")
            all_checks_passed = False
        else:
            size_mb = os.path.getsize(model_path) / (1024 * 1024)
            logger.info(f"✓ {model_name.capitalize()} model found: {model_path} ({size_mb:.1f} MB)")

    # Check hardware module availability
    try:
        hardware.initialize_mock_data()
        logger.info("✓ Hardware module initialized successfully")
    except Exception as e:
        logger.error(f"Hardware module initialization failed: {e}")
        all_checks_passed = False

    if all_checks_passed:
        logger.info("All startup checks passed")
    else:
        logger.error("Some startup checks failed - review errors above")

    return all_checks_passed


def load_models():
    """
    Loads the trained and quantized .tflite models into memory using the
    TensorFlow Lite interpreter for high efficiency on the Pi.
    """
    global thermal_interpreter, acoustic_interpreter
    logger.info("Loading AI models...")

    try:
        thermal_interpreter = tflite.Interpreter(model_path=config.THERMAL_MODEL_PATH)
        thermal_interpreter.allocate_tensors()

        # Log input/output details
        input_details = thermal_interpreter.get_input_details()
        output_details = thermal_interpreter.get_output_details()
        logger.info(f"Thermal model loaded: input shape {input_details[0]['shape']}, "
                   f"dtype {input_details[0]['dtype']}")
        logger.info(f"  Output shape {output_details[0]['shape']}, dtype {output_details[0]['dtype']}")

        acoustic_interpreter = tflite.Interpreter(model_path=config.ACOUSTIC_MODEL_PATH)
        acoustic_interpreter.allocate_tensors()

        input_details = acoustic_interpreter.get_input_details()
        output_details = acoustic_interpreter.get_output_details()
        logger.info(f"Acoustic model loaded: input shape {input_details[0]['shape']}, "
                   f"dtype {input_details[0]['dtype']}")
        logger.info(f"  Output shape {output_details[0]['shape']}, dtype {output_details[0]['dtype']}")

        return True

    except Exception as e:
        logger.critical(f"Failed to load one or more models: {e}")
        logger.critical("The application cannot run without the AI models")
        return False

def run_inference(interpreter, input_data):
    """
    A generic function to run inference on a loaded TFLite interpreter.
    Handles both float32 and int8 quantized models properly.
    """
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare the input tensor (needs to be in a batch of 1)
    input_tensor = np.expand_dims(input_data, axis=0)

    # Check if input shape matches model's expected shape
    expected_shape = input_details[0]['shape'][1:]
    if not np.array_equal(expected_shape, input_data.shape):
         logger.error(f"Input data shape {input_data.shape} does not match "
                     f"model expected shape {expected_shape}")
         return 0.0 # Return a non-anomaly score

    # Handle quantized input if needed
    input_dtype = input_details[0]['dtype']
    if input_dtype == np.int8:
        # Model expects INT8 input, but we keep input/output as float32
        # The interpreter handles conversion internally when configured properly
        input_tensor = input_tensor.astype(np.float32)
    elif input_dtype == np.uint8:
        input_tensor = input_tensor.astype(np.float32)
    else:
        input_tensor = input_tensor.astype(np.float32)

    try:
        interpreter.set_tensor(input_details[0]['index'], input_tensor)
        interpreter.invoke()
        prediction = interpreter.get_tensor(output_details[0]['index'])[0]

        # Handle quantized output if needed
        output_dtype = output_details[0]['dtype']
        if output_dtype in [np.int8, np.uint8]:
            # Dequantize if output is quantized (though we configured for float32 output)
            scale, zero_point = output_details[0]['quantization']
            if scale != 0:
                prediction = scale * (prediction.astype(np.float32) - zero_point)

        # Assuming the model's output is a single value from a sigmoid function
        return float(prediction[0])

    except Exception as e:
        logger.error(f"Inference error: {e}")
        return 0.0

def main():
    """
    The main function that orchestrates the Premonitor system.
    """
    logger.info("="*60)
    logger.info("Starting Premonitor System")
    logger.info("="*60)

    # --- Initialization ---
    # Run startup checks
    if not startup_check():
        logger.critical("Startup checks failed - cannot continue")
        sys.exit(1)

    # Load models
    if not load_models():
        logger.critical("Model loading failed - cannot continue")
        sys.exit(1)

    # TODO: In a V2.0 product, load device-specific config from a JSON file.
    # For now, we use the defaults from config.py.

    # --- Main Monitoring Loop ---
    logger.info("Starting main monitoring loop")
    logger.info(f"Sensor read interval: {config.SENSOR_READ_INTERVAL}s")

    while True:
        try:
            current_time = datetime.now().strftime('%H:%M:%S')
            logger.debug(f"Check cycle at {current_time}")

            # 1. Read from sensors (using mock hardware for now)
            thermal_image = hardware.read_thermal_image()
            acoustic_spectrogram = hardware.read_acoustic_spectrogram()
            gas_reading = hardware.read_gas_sensor()

            # 2. Process data with AI models to get confidence scores
            thermal_score = run_inference(thermal_interpreter, thermal_image)
            acoustic_score = run_inference(acoustic_interpreter, acoustic_spectrogram)

            if config.DEBUG_MODE:
                logger.debug(f"Scores - Thermal: {thermal_score:.3f}, "
                           f"Acoustic: {acoustic_score:.3f}, Gas: {gas_reading:.1f}")

            # 3. Apply Sensor Fusion & Alerting Logic
            # This is the core intelligence of the system.

            # Case 1: High-confidence single-sensor alert (critical)
            if thermal_score > config.THERMAL_ANOMALY_CONFIDENCE:
                details = f"A critical thermal anomaly was detected with a high confidence score of {thermal_score:.3f}."
                logger.warning(f"CRITICAL: High-Confidence Thermal Anomaly (score={thermal_score:.3f})")
                alert_manager.send_alert_in_background("CRITICAL: High-Confidence Thermal Anomaly", details)

            elif acoustic_score > config.ACOUSTIC_ANOMALY_CONFIDENCE:
                details = f"A critical acoustic anomaly was detected with a high confidence score of {acoustic_score:.3f}."
                logger.warning(f"CRITICAL: High-Confidence Acoustic Anomaly (score={acoustic_score:.3f})")
                alert_manager.send_alert_in_background("CRITICAL: High-Confidence Acoustic Anomaly", details)

            # Case 2: Correlated multi-sensor alert (proactive)
            elif thermal_score > config.FUSION_CORRELATION_CONFIDENCE and acoustic_score > config.FUSION_CORRELATION_CONFIDENCE:
                details = (f"A potential issue was detected by correlating multiple weak signals.\n"
                           f"- Thermal anomaly score: {thermal_score:.3f} (Threshold: {config.FUSION_CORRELATION_CONFIDENCE})\n"
                           f"- Acoustic anomaly score: {acoustic_score:.3f} (Threshold: {config.FUSION_CORRELATION_CONFIDENCE})")
                logger.warning(f"WARNING: Correlated anomaly (thermal={thermal_score:.3f}, acoustic={acoustic_score:.3f})")
                alert_manager.send_alert_in_background("WARNING: Correlated Anomaly Detected", details)

            # Case 3: Simple threshold alert for basic sensors
            if gas_reading > config.GAS_ANALOG_THRESHOLD:
                details = f"The gas sensor reading was {gas_reading:.1f}, which is above the threshold of {config.GAS_ANALOG_THRESHOLD}."
                logger.warning(f"WARNING: High gas level detected (reading={gas_reading:.1f})")
                alert_manager.send_alert_in_background("WARNING: High Gas Level Detected", details)

            # 4. Wait for the next cycle
            time.sleep(config.SENSOR_READ_INTERVAL)

        except KeyboardInterrupt:
            logger.info("Shutdown signal received - exiting gracefully")
            break
        except Exception as e:
            logger.error(f"Unhandled error in main loop: {e}", exc_info=True)
            # In a real product, you might want to send a "System Health" alert here.
            logger.info("Waiting 30s before retry to avoid error spam")
            time.sleep(30)

if __name__ == "__main__":
    main()